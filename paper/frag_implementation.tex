\section{Implementation}
% Section intro:
% 1. Expectations (what hosts are there, and which ones run our code?)
% 2. Overview of the distinct pieces of software (client kernel, client daemon,
%    detour daemon)
% Subsection - Detour daemon
% Subsection - Client kernel
% Subsection - Client daemon

% smoother transition once we have everything

The implementation of this project is based on the following expectations.
First, the server must support Multipath TCP. However, beyond this, no other
modifications should be required of the server. Second, the client and detour
point are assumed to be ``active'' participants in the collective, and so they
may run whatever software is necessary to participate.

The client must run a modified kernel, which supports creating MPTCP subflows
across detours. In addition, the client must also run a daemon which discovers
and negotiates with detour points, finally reporting available ones to the
kernel. These concerns remain separate for several reasons. First, communication
with detour points requires communication over the Internet. While it is
possible to do user-level networking within the kernel, it is much simpler and
more future-proof to use the userspace socket API. Second, one type of detour
requires starting an OpenVPN process, an operation which should only be done
from userspace. Finally, userspace tools may be more easily configured and can
perform a more broad spectrum of computations, such as floating point math.

The detour need not support Multipath TCP. However, it must run one of two
possible daemons, depending on which type of detour it will be hosting.

\subsection{Detour Daemon}

In this project, we implement two different mechanisms for tunneling MPTCP
subflows across a detour. In both cases, the detour host must run a daemon that
assists in tunneling traffic from the client to the server.

\subsubsection{OpenVPN Tunneling}

The first mechanism involves the open-source tool OpenVPN. Detours run a
specially configured OpenVPN server. Clients connect to this VPN, creating a
virtual network device. The client receives a routing rule from the detour
advertising that it can reach all hosts on the Internet with a very high cost.
As a result, the client operating system will prefer other routes to the VPN.

The VPN connection is configured as follows:

\begin{itemize}
\item The connection is over UDP, to avoid the so-called ``TCP Meltdown'' effect
  caused by two congestion control algorithms interfering with each other.
  % TODO citation
\item The initial connection negotiation requires the client and server to
  exchange certificates. OpenVPN allows the user to specify the PKI root. A
  centralized detour management system could authorize clients and servers by
  signing certificates, enabling tunneling for members and preventing it for
  non-members.
\item Subsequent messages are not protected by encryption or signatures, to
  avoid the computational overhead.
\end{itemize}

One important aspect of VPN configuration is that the detour must create a
private IP subnet. It will provide a DHCP service in order to assign IP
addresses to clients as they connect to the VPN. Clients are expected to be able
to connect to multiple detours simultaneously. In order to do this without
conflict, each detour must use a distinct private IP subnet, so that there is no
chance of the client addresses overlapping, and also so that the gateway address
of the VPN is unique on the client.

In a large-scale implementation of this technology, these subnet allocations
could be handled by a centralized detour management server. The 10.0.0.0/8
subnet is reserved for private networks, and if each detour were to establish
its own /24 subnet, there would be capacity for 65,536 non-conflicting subnets.
However, in our testing, subnets were manually allocated to avoid conflicts.

Finally, in order to forward VPN traffic to the internet, the detour must be
configured (via Netfilter) to perform network address translation (NAT) on
outgoing packets. This process is typical for a home router and for VPNs which
provide Internet access. Multipath TCP is designed with this behavior in mind,
and so it is capable of functioning through NAT, so long as the middleboxes do
not remove TCP options in transit.

% TODO cite MPTCP design/architecture for middleboxes

The OpenVPN implementation has several attractive qualities. First, it relies on
a well-used protocol for forwarding traffic. Second, it need not be configured
on a per-connection basis. Finally, it provides a built in mechanism for
mutually authenticating the client and detour. However, there are some
drawbacks. Since packets are tunneled, at least 28 bytes of overhead (IP and UDP
headers) are required. To avoid fragmentation, the connection's maximum segment
size (MSS) must be adjusted.

\subsubsection{NAT Tunneling}

The second approach directly modifies packets, without actually using a protocol
(like OpenVPN) for tunneling. Since we only need to forward some connections to
some destinations, the full power of OpenVPN is not necessary. Instead, packets
are addressed directly to the detour. The detour can forward these to the
final destination, and forward reply packets to the client. In order to do this,
the detour must have advance knowledge of the final destination of the packet.
So, clients use a simple signaling protocol to request tunnels via a detour.

The Linux kernel's Netfilter framework allows for two types of NAT. The first
kind is source NAT, or SNAT, rewrites the source address of the packet. This is
the typical form of NAT used by home routers: the source address of an outgoing
connection is rewritten to be the router's external IP address, but the
destination is unchanged. The second kind of NAT supported by Netfilter is
destination NAT, or DNAT. This rewrites the destination address of the packet.

% TODO - netfilter docs citation

The detour takes advantage of both forms of NAT offered by the kernel. First, it
applies DNAT, setting the destination address to be the final server's IP
address (arranged ahead of time). Then, it applies SNAT, setting the source
address to be its own external IP. The Netfilter framework remembers these
connections so that reply packets are properly forwarded back to the client.

Thanks to Netfilter, this NAT mapping can be arranged with two \texttt{iptables}
commands, with no custom kernel or user-space code required for packet
forwarding. It is possible to implement this in userspace using raw sockets, but
this approach has several drawbacks. It would have to be single-threaded, and it
would require that packet data be copied from kernel space into user space and
back. More critically, the connection tracking provided by the kernel is more
robust than what would be implemented by user space.

As mentioned earlier, the server address must be communicated to the detour
ahead of time in order to use this scheme. To that end, we have designed a
simple UDP-based for clients to ``request'' NAT mappings from a detour. The
detour listens for requests on UDP port 45672. The request format is shown
below.

\begin{lstlisting}
+-----------------------------------+
| ver(1) | op(1)  | reserved (2)    |
+-----------------------------------+
|           rip (4 bytes)           |
+-----------------------------------+
| rpt (2 bytes)   | dpt (2 bytes)   |
+-----------------|-----------------+
\end{lstlisting}

The \texttt{ver} field is a protocol version, currently set to 1. The
\texttt{op} field contains the operation of this message: request (0) or
response (1). The \texttt{reserved} field is unused, currently existing to pad
the request. The field \texttt{rip} (short for ``remote IP'') is the address
that the client wishes to create a detour to. \texttt{rpt} is the TCP port on
the server to create a detour to. To understand the \texttt{dpt} field, it is
useful to describe all of the addresses and ports in use during a tunneled
connection:
\begin{itemize}
\item Client IP, port: the IP address of the client, and the client-side port,
  which is usually ephemeral.
\item Detour IP, port (client side): the IP and port that the client will
  address its communication to.
\item Detour IP, port (remote side): the IP and port which are used when the
  detour opens its corresponding connection on the remote.
\item Remote IP, port: the IP and port which the client is tunneling its
  connection to.
\end{itemize}

The \texttt{dpt} field corresponds to the detour port, client side. In a
request, the client proposes a port. In the response, the detour specifies the
actual port which will be used. The server may wish to alter the \texttt{dpt}
field for several reasons:
\begin{itemize}
\item A tunnel to the remote IP, port pair is already open on a different detour
  port.
\item The client already has a detour open (to a different remote) on the given
  port.
\item The detour may have a socket accepting connection on the proposed port,
  and so it may wish to avoid creating a tunnel which would prevent the client
  from accessing that service.
\end{itemize}

In order to create the tunnels, the detour runs two IPTables commands, which
create NAT rules. The first performs DNAT:

\begin{lstlisting}
iptables -t nat -A PREROUTING \
         -s (*\emph{ClientAddress}*) \
         -d (*\emph{DetourAddress}*) \
         -p tcp --dport (*\emph{DetourPort}*) \
         -j DNAT --to (*\emph{RemoteAddress}*):(*\emph{RemotePort}*)
\end{lstlisting}

In this command, we add to the \texttt{nat} table, on the \texttt{PREROUTING}
chain. Incoming packets from the client, addressed to the agreed upon
\textit{DetourPort} are sent to the \texttt{DNAT} chain, which rewrites the
destination address to be the agreed upon remote address and port. This must
occur before the kernel routes the packet. Once the kernel has routed the
packet, the DNAT rule can be applied:

\begin{lstlisting}
iptables -t nat -A POSTROUTING \
         -s (*\emph{ClientAddress}*) \
         -d (*\emph{RemoteAddress}*) \
         -p tcp --dport (*\emph{RemotePort}*) \
         -j SNAT --to (*\emph{DetourAddress})
\end{lstlisting}

Since the destination address and port have already been modified, this rule
looks for packets addressed to the remote address and port, but still using the
client address as the source. It jumps these packets to the DNAT chain, where
their source address will be rewritten as the detour's address.

It is important to understand that these rules are generally only applied to the
initial SYN segment of the TCP flows. Once these rules are applied, the
connection tracking mechanism of the kernel takes over, ensuring that these
rules are applied in reverse to response packets.

The detour daemon is implemented as a Python script. It listens for tunnel
requests, executes the correct commands to create them, and then sends
responses. It ensures that detour ports do not collide with each other, or with
ports that are open on the daemon machine. It also ensures that on termination,
all tunnels are torn down.

This mechanism has the advantage that it requires zero overhead. Since no
headers are added, the full amount of data may be transmitted on these segments.
However, its disadvantage is that tunnels must be configured each time a new
connection is created on the client. While the client may simply send packets to
a new remote through an OpenVPN tunnel, it must incur a RTT delay requesting a
new tunnel from the NAT detour before it can begin establishing a subflow.

\subsection{Client Daemon}

In order for the client to use detours, it must either initiate OpenVPN
connections with them, or dynamically request tunnels via the UDP protocol. In
either case, a client program must manage these activities. In particular, a
program must be able to notify the kernel when new detours are available for
immediate use, and it must be able to respond to requests from the kernel for
new detours. The mechanism for communication with the kernel will be discussed
in the following section.

\subsubsection{OpenVPN Client Management}

The client daemon reads a configuration file which informs it of detours which
use OpenVPN. On startup, it initiates an OpenVPN connection with each detour.
Once the connections are established, it sends a message to the kernel with the
virtual network device's name.

\subsubsection{NAT Client Management}

In order to use NAT tunnels, the client daemon must be much more complex. The
client waits for kernel tunnel requests. Upon receiving these requests, it sends
a UDP request to each NAT detour server specified in its configuration file. It
then waits for responses from the detour servers. Upon receiving each response,
it sends a reply back to the kernel notifying it of the new NAT tunnel.

\subsection{Client Communication}

In order to communicate between kernel and userspace, we use Generic Netlink
sockets. Netlink is an address family for the socket interface
(\texttt{AF\_NETLINK}, similar to \texttt{AF\_INET}), which allows just this
sort of communication. There are several types of Netlink socket, but the
Generic Netlink type allows any kernel component to define its own protocol,
while still managing marshalling data.

Our protocol is similar to the UDP protocol for communication between the client
and detour. The kernel sends a message of type \texttt{DETOUR\_C\_REQ} to a
``multicast group'' which the client daemon subscribes to. This message contains
the necessary parameters (remote address, remote port). The client daemon may
reply to the kernel with \texttt{DETOUR\_C\_ADD} commands, which add detour
entries, and \texttt{DETOUR\_C\_DEL} commands, which remove them.

The protocol allows the client to register both OpenVPN and NAT based detours,
both of which are supported in the kernel implementation.

\subsection{Client Kernel}

Finally, the client kernel must be capable of requesting detours, receiving
information from userspace, and using that information to establish subflows.
Support for this functionality is implemented as a Linux kernel module. In
particular, it is implemented as a MPTCP ``path manager.'' A path manager is a
kernel module which makes decisions regarding when to advertise additional
addresses and when to create new subflows. An instance of the path manager is
created for each MPTCP connection, when the connection becomes fully established
(that is, after the third packet in the handshake).

Upon creation, our path manager enqueues a task to be executed in a long-running
kernel thread. This task first sends a message to userspace on the Generic
Netlink socket, requesting a detour. Then, the task looks through an internal
list of detours. The kernel maintains two lists of detours. The first is a list
of OpenVPN detours. These detours are not specific to the remote port and host.
The second list contains NAT tunnels which were created by the client daemon and
sent to the kernel via the \texttt{DETOUR\_C\_ADD} command.

The task attempts to add up to $N$ subflows, where $N$ is a configurable limit.
If it cannot (due to lack of subflows), it terminates. If OpenVPN detours are
available, then they take precedence over the NAT detours, because they are
available immediately.

When the client receives a \texttt{DETOUR\_C\_ADD} command, it first adds the
details of the tunnel to its internal list. Then, if the tunnel was a NAT
tunnel, it wakes all the path managers so that they can attempt to create a
subflow through that tunnel.
